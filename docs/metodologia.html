<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Metodolog√≠a de IA</title>
</head>

<body>
    <p><a href="index.html">&laquo; √çndice</a></p>
    <h1>Metodolog√≠a de IA</h1>

    <p>
        Existen un conjunto de metodolog√≠as y enfoques que pueden apoyar a este problema, en particular, debido al
        conjunto de im√°genes encontrados de entrenamiento [9], se pens√≥ en una estructuraci√≥n robusta del modelo, por lo
        que se optar√° por utilizar un modelo de Vision Transformer (ViT) que busca tener un comportamiento parecido al
        Transformer implementado en procesamiento de lenguaje natural, pero para im√°genes. En espec√≠fico, se buscar√°
        realizar un Fine Tuning a un modelo YOLO, es decir realizar un entrenamiento sobre un modelo pre-entrenado.
    </p>

    <h2>ViT.</h2>

    <p>
        Para realizar el entrenamiento de un modelo ViT, se mostrar√° de manera secuencial, qu√© recorrido realiza a una
        imagen en particular.
    </p>

    <ol>
        <li>
            <strong>Patching.</strong>
            Divide la imagen en ‚Äúparches‚Äù, con un tama√±o de parches 16 x 16 y son aplanados en un vector 16 x 16 x C,
            siendo C, el n√∫mero de canales de la imagen. Por ejemplo: Si la imagen es de 224 x 244 RGB, se obtienen 196
            parches en total [10] con dimensi√≥n 768.
        </li>
        <br>
        <li>
            <strong>Embedding.</strong>
            Similar al comportamiento de los transformers, a cada parche se le hace un embedding lineal de dimensi√≥n D,
            donde D es un hiper par√°metro del modelo ViT, y es el que representa la capacidad del modelo de adquirir
            caracter√≠sticas complejas o con im√°genes con ruido en una imagen [11]. Los valores usuales en D, var√≠an
            dependiendo la configuraci√≥n inicial de ViT: ViT-Base: ùê∑=768; Para ViT-Large: ùê∑=1024; Para ViT-Huge: ùê∑ =
            1280 [12].
        </li>
        <br>
        <li>
            <strong>Adici√≥n del Token de Clasificaci√≥n.</strong>
            A cada vector embeding se le concatena al inicio un token de clasificaci√≥n (CLS) inicialmente dado por cero,
            es token cambia a medida que se va transcurriendo el entrenamiento [10].
        </li>
        <br>
        <li>
            <strong>Adici√≥n de Positional Encodings.</strong>
            A cada vector embeding con token CLS, se suma un vector posicional para que el modelo pueda aprender la
            posici√≥n relativa de cada parche en la imagen [13].
        </li>
        <br>
        <li>
            <strong>Entrada al Transformer Encoder.</strong>
            Cada vector embeding con token CLS sumado con su vector posicional se introduce hacia el encoder transformer
            que est√° compuesta por capas de Self.Atenttion, Multi-Head y feed-forward totalmente conectadas:

            <ul>
                <li>-> Self.Atenttion y Multi-Head: Permite al modelo aprender relaciones de largo alcance entre los
                    parches. a trav√©s de una matriz de atenci√≥n que representa la importancia de cada parche con
                    respecto a los dem√°s.</li>
                <li>-> Feed-forward: Luego de aplicar una normalizaci√≥n por capas a los resultados del √≠tem anterior, se
                    pasa a trav√©s de una red neuronal completamente conectada, normalmente con una funci√≥n de activaci√≥n
                    no lineal (ReLU o GELU), seguida de otra normalizaci√≥n por capas y conexi√≥n residual [10]. Este
                    proceso se repite por cada capa en el encoder.</li>
            </ul>
        </li>
        <br>
        <li>
            <strong>Extracci√≥n del Token de Clasificaci√≥n.</strong>
            Luego de pasar por cada una de las capas del encoder (12-14 capas [10]), a cada vector embedding con token
            CLS sumado con su vector posicional, se obtiene el token CLS que pasa por una capa Softmax para obtener la
            probabilidad de pertenecer a una clase [14].
        </li>
        <br>
        <li>
            <strong>Actualizaci√≥n de Pesos y Optimizaci√≥n.</strong>
            Con la salida del modelo se comparan las etiquetas verdaderas de la imagen para calcular el valor de la
            funci√≥n de p√©rdida, se realiza retropropagaci√≥n para ajustar los pesos del modelo utilizando un optimizador
            (por ejemplo, Adam o SGD), sos pesos del modelo se actualizan utilizando algoritmos de optimizaci√≥n basados
            en el gradiente (como Adam o SGD), y se repiten los pasos de forward pass y backpropagation para cada lote
            de datos hasta que se complete el n√∫mero de √©pocas de entrenamiento [15].
        </li>

    </ol>
    <br>
    <p>Se puede apreciar la arquitectura de ViT en la siguiente imagen, sacada del paper original [10]</p>

    <figure>
        <img src="images/vit_architecture.jpg" alt="arquitectura-ViT" width="900" height="500">
        <figcaption><strong>Arquitectura ViT</strong></figcaption>
    </figure>



    <h2>YOLOS.</h2>

    <p>En este caso, se realiz√≥ la implementaci√≥n de Fine-Tunning a dos arquitecturas de YOLOS: </p>

    <ol>
        <li>Usando el modelo ‚Äúhustvl/yolos-small‚Äù de la librer√≠a YolosImageProcessor de transformers [16]. Utilizando
            fotos de carreteras de Jap√≥n, Noruega e India disponibles en [9], dicha arquitectura se le har√° referencia
            como yolos-small.</li>
        <br>
        <li>Usando el modelo ‚Äúyolov8s.pt‚Äù, utilizando todas las fotos disponibles para entrenar en [9].</li>
    </ol>

    <p>Antes de explicar la arquitectura del modelo, se deben establecer las diferencias entre los modelos trabajados,
        debido a que sus estructuras son semejantes, poseen enfoques diferentes que afectan desde su procesamiento hasta
        su entrenamiento, dichas diferencias se mostrar√°n en la siguiente tabla.</p>


    <table border="1">
        <caption><strong>Diferencias entre yolov8s y yolos-small</strong></caption>
        <tr>
            <th>Nombre</th>
            <th>Arquitectura</th>
            <th>Aplicaci√≥n</th>
        </tr>
        <tr>
            <td>yolov8s</td>
            <td>Es la √∫ltima iteraci√≥n del modelo YOLO a la fecha, donde se enfoca en la eficiencia y uso de la CNNs
                para extracci√≥n de caracter√≠sticas [17].</td>
            <td>Ideal para aplicaciones en tiempo real y casos donde se necesita un buen equilibrio entre precisi√≥n y
                velocidad [17].</td>
        </tr>
        <tr>
            <td>yolos-small</td>
            <td>M√°s parecido a la estructura inicial de YOLOS buscando la manera de detectar como DETR [16]</td>
            <td>Aprovecha la capacidad de los transformers para modelar relaciones espaciales complejas [16].</td>
        </tr>


    </table>

    <p>Teniendo presente las diferencias anteriores, se mostrar√°, con una imagen en particular, cu√°l es el proceso de
        aprendizaje de esta arquitectura:</p>

    <ol>
        <li><strong>Preparaci√≥n de Datos.</strong>Se debe contar con una imagen y, las clases y bounding boxes (con
            formato x, y, w, h, estandarizados a la imagen original) correspondientes de dicha imagen. Estas tendr√°n un
            tama√±o fijo de resoluci√≥n, desde 512 x 512 hasta 874 x 874 [17].</li>
        <br>
        <li><strong>Divisi√≥n de la Imagen en Cuadr√≠culas.</strong>Divisi√≥n de la Imagen en Cuadr√≠culas.</li>
        <br>
        <li><strong>Forward Pass (Pasada hacia Adelante). </strong>Con cada una de las celdas de la imagen se establece
            una tuber√≠a de transformaci√≥n, donde la diferencia entre yolov8s y yolos-small es que en el Head, antes de
            obtener predicciones, pasa por m√°s capaz de convoluci√≥n [17] buscando realizar un barrido m√°s detallado
            buscando predecir la clase con una mayor confianza. A√∫n con esto ambos modelos realizar el siguiente Forward
            Pass:
            <ol type="a">
                <li><strong>Backbone.</strong>Est√° compuesto por un conjunto de capas de redes neuronales
                    convolucionales que convierte cada celda de la imagen en mapas de caracter√≠sticas que capturan
                    informaci√≥n sobre patrones, texturas y formas en diferentes niveles de profundidad [16]. </li>
                <li><strong>Neck</strong>A cada vector de caracter√≠sticas aplanado de cada celda de la imagen se le
                    realiza un path aggregation buscando mejorar la capacidad de la red para detectar objetos de
                    diferentes escalas y combinar caracter√≠sticas de diferentes niveles de la red backbone [16] </li>
                <li><strong>Head. </strong>Con dicho vector de caracter√≠sticas, luego de la pasada de Neck de cada celda
                    de la imagen se realiza la regresi√≥n de las coordenadas del bounding box (x, y, w, h), calcula el
                    valor de confianza con respecto a la contenci√≥n de una clase de inter√©s en la bounding box, y
                    predice la clase de cada uno de los bounding box [17].</li>
            </ol>
        </li>
        <br>
        <li><strong>Backpropagation (Retropropagaci√≥n) y Optimizaci√≥n.</strong>Con la triada de valores (Coordenadas del
            bounding box, clase, valor de confianza), se realiza el c√°lculo de los errores respectivos: desviaci√≥n
            est√°ndar en el centro, ancho y altura del bounding box con respecto a las reales, el error en la
            probabilidad de clase predicha comparada con la clase verdadera del objeto y la confianza predicha comparada
            con la puntuaci√≥n real. La funci√≥n de p√©rdida global suma estos tres valores. Posteriormente, luego de
            acumular todos los valores de p√©rdidas globales se calculan los gradientes con respecto a los pesos de la
            red utilizando el algoritmo de retropropagaci√≥n y los pesos de la red se actualizan utilizando un
            optimizador (como Adam o SGD) para minimizar la funci√≥n de p√©rdida [16].
        </li>

    </ol>





    <p><a href="index.html">&laquo; √çndice</a></p>
</body>

</html>