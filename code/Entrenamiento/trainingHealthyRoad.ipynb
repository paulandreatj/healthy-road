{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "from datetime import datetime\n",
    "import json\n",
    "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
    "\n",
    "import torchvision\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import ImageFont\n",
    "from transformers import YolosForObjectDetection, YolosImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH=\"RDD2022/Img\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de entrenamiento.\n",
    "\n",
    "Se realizaron dos implementaciones de Fine-Tunning:\n",
    "\n",
    "1. Modelo \"hustvl/yolos-small\" con 20 epochs utilizando las imagenes de Japón, Índia y Noruega.\n",
    "2. Modelo \"yolov8s.pt\" con 100 epochs utilizando todas las imagenes disponibles en el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo \"yolov8s.pt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtención de data a un dataframe.\n",
    "\n",
    "Se realizó un desarrollo para:\n",
    "\n",
    "1. Obtener todas las posibles rutas a las imagenes disponibles.\n",
    "2. El conjunto de datos, posee una carpeta con las imágenes y otra carpeta donde están las anotaciones de interés, por lo que se realizó una función para, dado un archivo .xml, obtener la clase, las coordenadas del bounding box (x1, y1, x2, y2), los valores de resolución de la imagen (Width, Height)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Obtener todas las posibles rutas.\n",
    "\n",
    "Al descomprimir el .zip de 12 GBs nombrado como RDD2022_released_through_CRDDC2022.zip y descomprimirlo, obtendrás carpetas repartidas por el país en que fue tomada la imagen, por lo que la siguiente función es para obtener la ruta absoluta desde la ruta raíz en que fue descomprimido la carpeta. En este caso, el comprimido fue extráido dentro de la carpeta RDD2022/Img, por lo que `root_path=\"RDD2022/Img\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RDD2022/Img/China_Drone/train/annotations/xmls/',\n",
       " 'RDD2022/Img/China_Drone/train/images/China_Drone_000000.jpg/',\n",
       " 'RDD2022/Img/China_Drone/train/images/China_Drone_000001.jpg/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paths_files(root_path, constraint=\"\", search_img=\"\"):\n",
    "    \"\"\"\n",
    "        ** root_path:  Nombre del folder donde estan todas las carpetas por pais del comprimido dado por RDD2022_released_through_CRDDC2022.zip\n",
    "        ** constraint: Filtrar la lista de todos los path dependiendo lo que contenga cada valor de la lista.\n",
    "            Ej:\n",
    "                get_paths_files(root_path=\"Img\", constraint=\"train\"), te dara todas las rutas donde aparezca,\n",
    "                la palabra train.\n",
    "\n",
    "    \"\"\"\n",
    "    #Ruta raiz de la carpeta de paises\n",
    "    path_folder_country=root_path\n",
    "\n",
    "    #Lista total de paths con imagenes y anotaciones\n",
    "    l=[root_path+\"/\"+ctry_name+\"/\"+folder_cont_tra_tes+\"/\"+annot_imgs+\"/\"+value_annot_imgs+\"/\"\n",
    "       #+value_annots\n",
    "        for ctry_name in os.listdir(path_folder_country)\n",
    "            for folder_cont_tra_tes in os.listdir(path_folder_country+\"/\"+ctry_name)\n",
    "                for annot_imgs in os.listdir(path_folder_country+\"/\"+ctry_name+\"/\"+folder_cont_tra_tes)\n",
    "                    for value_annot_imgs in os.listdir(path_folder_country+\"/\"+ctry_name+\"/\"+folder_cont_tra_tes+\"/\"+annot_imgs)\n",
    "                        #for value_annots in os.listdir(path_folder_country+\"/\"+ctry_name+\"/\"+folder_cont_tra_tes+\"/\"+annot_imgs+\"/\"+value_annot_imgs)\n",
    "                        ]\n",
    "    # Filtrar por palabra clave\n",
    "    if(len(constraint)!=0):\n",
    "        r = filter(lambda p: constraint in p,l)\n",
    "        l=list(r)\n",
    "\n",
    "    # Filtrar por nombre de la imagen de la carpeta train\n",
    "    if(len(search_img)!=0):\n",
    "        r = filter(lambda p: search_img in p,l)\n",
    "        l=list(r)\n",
    "\n",
    "    return l\n",
    "\n",
    "get_paths_files(ROOT_PATH)[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Obtener todas las rutas a los archivos .xml.\n",
    "\n",
    "Para obtener la ruta de los xml, se utilizó la función anterior, junto con una configuración de esta que permite filtrar por los valores de la lista, esto se logra cambiando el parámetro *constraint*, haciendo `get_paths_files(ROOT_PATH,constraint=\"xml\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RDD2022/Img/China_Drone/train/annotations/xmls/China_Drone_000000.xml',\n",
       " 'RDD2022/Img/China_Drone/train/annotations/xmls/China_Drone_000001.xml',\n",
       " 'RDD2022/Img/China_Drone/train/annotations/xmls/China_Drone_000002.xml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_xml_files():\n",
    "    file_paths_xml=get_paths_files(ROOT_PATH,\"xml\")\n",
    "    return [folder_xml+file_xml for folder_xml in file_paths_xml for file_xml in os.listdir(folder_xml)]\n",
    "\n",
    "get_xml_files()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Obtener los datos de las anotaciones de cada imagen.\n",
    "\n",
    "Para obtener los valores usando los encabezados de los archivos .xml, se utilizó la siguiente función apoyandose de la librería ` xml.etree.ElementTree`, que convierte la estructura del archivo en un árbol que posee clave: valor, para poder acceder a este a través de la jerarquía de los encabezados.\n",
    "\n",
    "Los valores que devolverá serán de la siguiente forma en una tupla:\n",
    "\n",
    "```\n",
    "('RDD2022/Img/China_Drone/train/images/China_Drone_000001.jpg',\n",
    " ['D10', 'D00'],\n",
    " [[0.20703125, 0.1650390625, 0.359375, 0.056640625],\n",
    "  [0.2099609375, 0.376953125, 0.048828125, 0.30859375]],\n",
    " 512,\n",
    " 512)\n",
    "```\n",
    "\n",
    "Notemos que tanto las clases como los bounding boxes normalizados están en listas y que cada índice estas, corresponden entre sí, es decir: El bounding box asociado a D10 es [0.20703125, 0.1650390625, 0.359375, 0.056640625] y para D00 es [0.2099609375, 0.376953125, 0.048828125, 0.30859375]. Se decidió esta estructura debido a que se asemeja a las usadas al COCO dataset benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener el nombre del archivo sin la extensión.\n",
    "def get_name_file(path):\n",
    "    l=path.split(\"/\")\n",
    "    return l[len(l)-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RDD2022/Img/China_Drone/train/images/China_Drone_000001.jpg',\n",
       " ['D10', 'D00'],\n",
       " [[0.20703125, 0.1650390625, 0.359375, 0.056640625],\n",
       "  [0.2099609375, 0.376953125, 0.048828125, 0.30859375]],\n",
       " 512,\n",
       " 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_meta_data_xml(path_xml):\n",
    "    tree = ET.parse(path_xml)\n",
    "    # Obtener el arbol\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # list_tuples=[]\n",
    "\n",
    "    # Obtener size\n",
    "    W = int(root.find(\"size\").find(\"width\").text)\n",
    "    H = int(root.find(\"size\").find(\"height\").text)\n",
    "\n",
    "    list_labels=[]\n",
    "    list_bboxes=[]\n",
    "    # Por cada objeto encontrado\n",
    "\n",
    "    #Anadir la imagen a la lista de imagenes\n",
    "    name_file=get_name_file(path_xml)\n",
    "    path_img=get_paths_files(ROOT_PATH,\"images\",name_file)[0]\n",
    "    \n",
    "    for obj in root.findall(\"object\"):\n",
    "\n",
    "        #Obtener los datos necesarios: label, y para armar la caja\n",
    "        label=obj.find(\"name\").text\n",
    "        list_labels.append(label)\n",
    "\n",
    "        xmin=float(obj.find(\"bndbox\").find(\"xmin\").text)\n",
    "        ymin=float(obj.find(\"bndbox\").find(\"ymin\").text)\n",
    "        xmax=float(obj.find(\"bndbox\").find(\"xmax\").text)\n",
    "        ymax=float(obj.find(\"bndbox\").find(\"ymax\").text)\n",
    "\n",
    "        # Convertir a (x, y, w, h)\n",
    "        x = (xmin + xmax) / 2.0\n",
    "        y = (ymin + ymax) / 2.0\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "\n",
    "        # Normalizar\n",
    "        x_norm = x / W\n",
    "        y_norm = y / H\n",
    "        w_norm = w / W\n",
    "        h_norm = h / H\n",
    "\n",
    "        list_bboxes.append([x_norm, y_norm, w_norm, h_norm])\n",
    "\n",
    "    return (path_img[0:len(path_img)-1],list_labels,list_bboxes, W, H)\n",
    "\n",
    "xml_aux=get_xml_files()[0:3]\n",
    "get_meta_data_xml(xml_aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Creación del dataframe.\n",
    "\n",
    "El siguiente código es costoso debido a la cantidad de imágenes totales involucradas. Por cada .xml, se realiza un registro a un dataframe con columnas `\"Path\",\"L_Label\",\"L_Bbox\",\"Width\",\"Height\"`, llegando a un total de 38384, de los cuáles, eliminando registros que no poseían Label ó bounding box, terminaría siendo 26 mil imágenes aproximadamente para usarse. \n",
    "\n",
    "Tardó cerca de 1 hora en procesar las 38 mil imágenes e ingresar los registros en un dataframe. Luego de eliminar aquellas imagenes que no tenían clasificación, se guardó el dataframe en un archivo `df_acum_notEmpyBbox.xlsx`, por lo que se convirtió en un checkpoint para el desarrollo posterior, teniendo que cargar dicho archivo en lugar de correr este código.\n",
    "\n",
    "```python\n",
    "# Inicializacion de dataframe\n",
    "df_metadata_acum_2=pd.DataFrame(columns=[\"Path\",\"L_Label\",\"L_Bbox\",\"Width\",\"Height\"])\n",
    "\n",
    "# Obtener todas las rutas de los elementos xml que contiene labels y coordenadas\n",
    "xml_paths=get_xml_files()\n",
    "\n",
    "#Contador para ver si se esta agregando\n",
    "con=0\n",
    "\n",
    "# Por cada ruta del arxhico xml\n",
    "for xml_path in xml_paths:\n",
    "    # print(xml_path)\n",
    "    # Obtener sus metadatos por label y coordenadas\n",
    "    list_rows=get_meta_data_xml(xml_path)\n",
    "    \n",
    "    df_metadata_acum_2.loc[len(df_metadata_acum_2.index)] = list_rows\n",
    "\n",
    "    # Verificando con los primeros 100 paths si se esta cargando bien\n",
    "\n",
    "    #if(con==100):\n",
    "        #print(list_rows)\n",
    "    #    break\n",
    "    #con+=1\n",
    "\n",
    "df_metadata_acum_2.tail(4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataframe de las imagenes que tienen alguna clasificacion\n",
    "\n",
    "df_acum_ALL_notEmpty=pd.read_excel('df_acum_notEmpyBbox.xlsx')\n",
    "df_acum_ALL_notEmpty.drop([\"Unnamed: 0\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>L_Label</th>\n",
       "      <th>L_Bbox</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26657</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D10']</td>\n",
       "      <td>[[0.36171875, 0.78125, 0.0859375, 0.01875]]</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26658</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D00', 'D00']</td>\n",
       "      <td>[[0.784375, 0.859375, 0.3, 0.26875], [0.808593...</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26659</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D00', 'D00']</td>\n",
       "      <td>[[0.51875, 0.925, 0.1, 0.146875], [0.65625, 0....</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26660</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D00']</td>\n",
       "      <td>[[0.13125, 0.69375, 0.2625, 0.159375]]</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Path         L_Label  \\\n",
       "26657  Img/United_States/train/images/United_States_0...         ['D10']   \n",
       "26658  Img/United_States/train/images/United_States_0...  ['D00', 'D00']   \n",
       "26659  Img/United_States/train/images/United_States_0...  ['D00', 'D00']   \n",
       "26660  Img/United_States/train/images/United_States_0...         ['D00']   \n",
       "\n",
       "                                                  L_Bbox  Width  Height  \n",
       "26657        [[0.36171875, 0.78125, 0.0859375, 0.01875]]    640     640  \n",
       "26658  [[0.784375, 0.859375, 0.3, 0.26875], [0.808593...    640     640  \n",
       "26659  [[0.51875, 0.925, 0.1, 0.146875], [0.65625, 0....    640     640  \n",
       "26660             [[0.13125, 0.69375, 0.2625, 0.159375]]    640     640  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acum_ALL_notEmpty.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creación de carpetas con estructura requerida.\n",
    "\n",
    "Basado en el ejemplo de Fine-Tunning de la detección de basura del mar (https://developer.ibm.com/tutorials/awb-train-yolo-object-detection-model-in-python/), indica se se debe tener una carpeta \"datasets\" que contenga en su raíz, en este caso para entrenamiento:\n",
    "\n",
    "```\n",
    "datasetsFolder\n",
    "    -- Train\n",
    "        -- images\n",
    "            -- image1.jpg\n",
    "            -- image2.jpg\n",
    "            ...\n",
    "        -- labels\n",
    "            -- image1.txt\n",
    "            -- image2.txt\n",
    "            ...\n",
    "```\n",
    "\n",
    "En los archivos txt, deben aparecer la información del label codificado y posteriormente los valores x, y, w, h estandarizados con respecto a la imagen original. Por ejemplo, este sería el archivo para `China_Drone_000003.txt`:\n",
    "\n",
    "```txt\n",
    "0 0.2080078125 0.93359375 0.380859375 0.04296875\n",
    "0 0.6943359375 0.638671875 0.611328125 0.08984375\n",
    "1 0.46484375 0.291015625 0.15234375 0.578125\n",
    "```\n",
    "Para lograr esa estructura de archivos y carpetas, se realizó la siguiente implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Codificación de Labels.\n",
    "\n",
    "Algunos datos son necesarios para realizar un correcto Fine-tunning para el modelo YOLOS, en este caso, está la codificación de las clases, para ello, se realizó el siguiente código para conocer cuáles eran las posibles clases diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos los posibles valores de label\n",
    "\n",
    "# Lista de valores uniquos\n",
    "label_uniques=[]\n",
    "\n",
    "# Por cada fila del dataframe\n",
    "\n",
    "for i in range(0,len(df_acum_ALL_notEmpty)):\n",
    "    row=df_acum_ALL_notEmpty.iloc[i]\n",
    "    #Obtener la lista\n",
    "    list_labels=eval(row[\"L_Label\"])\n",
    "    for label in list_labels:\n",
    "        if(label not in label_uniques):\n",
    "            label_uniques.append(label)\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, hubo un error de preprocesamiento que posteriormente no se cambió y es la presencia de la clase `D0w0` en una única imagen, dicha clase sería `D00`. Sin embargo, observaremos que no afecto de manera significativa el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'D10',\n",
       " 1: 'D00',\n",
       " 2: 'D20',\n",
       " 3: 'Repair',\n",
       " 4: 'D40',\n",
       " 5: 'Block crack',\n",
       " 6: 'D44',\n",
       " 7: 'D01',\n",
       " 8: 'D11',\n",
       " 9: 'D43',\n",
       " 10: 'D50',\n",
       " 11: 'D0w0'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear diccionario con todos los posibles valores\n",
    "label2Value={i: label_uniques[i] for i in range(0,len(label_uniques))}\n",
    "value2Label={v: k for k,v in label2Value.items()}\n",
    "label2Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Configuración de archivo .yaml y convenciones.\n",
    "\n",
    "El punto de enlace entre las rutas de los archivos de entrenamiento y la configuración del momento de entrenamiento, es por medio de un archivo nombrado de la siguiente forma: `config.yaml`. En dicho archivo se especifican las rutas donde están las imagenes de train, test y val, en este caso, todas las imagenes de entrenamiento se usarán, por lo que, así se establezca en la configuración, no habrá imagenes en test y val.\n",
    "\n",
    "El contenido del archivo se crea con la siguiente implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Universidad Nacional de Colombia\\2024 - 1\\3009150 Redes Neuronales Artificiales y Algoritmos Bio-Inspirados\\TFinal\\E2\\CRDDC\\metricasIoUdataJapan\n",
      "['args.yaml', 'datasets', 'detr', 'df_acum_notEmpyBbox.xlsx', 'df_metadata_acum_ALL.xlsx', 'df_metada_acum.xlsx', 'functionalDataYolo.ipynb', 'functionalHealthyRoad.ipynb', 'imagen_con_bbboxes.jpg', 'metadata_damage.xlsx', 'metricasIoUdataJapan.png', 'metricasIoUdataJin.png', 'New folder', 'pielCocoDriloCarretera.jpeg', 'RDD2022', 'results.csv', 'roaddamages', 'runs', 'save_japan', 'save_jin', 'trainingHealthyRoad.ipynb', 'United_States_004802.txt', 'util.py', 'vias-dañadas.jpeg', 'weights', 'yolov8n.pt', 'yolov8s.pt', 'yolov8_DAMT.ipynb', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Establecer una ruta raiz\n",
    "\n",
    "# store working directory path as work_dir\n",
    "work_dir = \".\"\n",
    "\n",
    "# print work_dir path\n",
    "print(os.getcwd())\n",
    "\n",
    "# print work_dir contents\n",
    "print(os.listdir(f\"{work_dir}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./roaddamages/data_test_coco/config.yaml updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# contents of new confg.yaml file\n",
    "def update_yaml_file(file_path):\n",
    "    data = {\n",
    "        'path': f'{work_dir}/roaddamages/data_test_coco',\n",
    "        'train': 'train/images',\n",
    "        'val': 'train/images',\n",
    "        'test': 'test/images',\n",
    "        'names': label2Value\n",
    "    }\n",
    "\n",
    "    # ensures the \"names\" list appears after the sub/directories\n",
    "    names_data = data.pop('names')\n",
    "    with open(file_path, 'w') as yaml_file:\n",
    "        yaml.dump(data, yaml_file)\n",
    "        yaml_file.write('\\n')\n",
    "        yaml.dump({'names': names_data}, yaml_file) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = f\"{work_dir}/roaddamages/data_test_coco/config.yaml\" #.yaml file path\n",
    "    update_yaml_file(file_path)\n",
    "    print(f\"{file_path} updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Convenciones**.\n",
    "\n",
    "El archivo `config.yaml` creado, en este caso en particular, tiene el siguiente contenido.\n",
    "\n",
    "```yaml\n",
    "path: ./roaddamages/data_test_coco\n",
    "test: test/images\n",
    "train: train/images\n",
    "val: train/images\n",
    "\n",
    "names:\n",
    "  0: D10\n",
    "  1: D00\n",
    "  2: D20\n",
    "  3: Repair\n",
    "  4: D40\n",
    "  5: Block crack\n",
    "  6: D44\n",
    "  7: D01\n",
    "  8: D11\n",
    "  9: D43\n",
    "  10: D50\n",
    "  11: D0w0\n",
    "```\n",
    "\n",
    "Algo que **destacar**, es que, por fuera de la carpeta **roaddamages**, debe llamarse **datasets** también, esto es por una configuración en la API a la hora de realizar el entrenamiento y buscar las imágenes. Dejando la estructura de la carpepta de la siguiente manera.\n",
    "\n",
    "```txt\n",
    "datasets                    (Obligatorio)\n",
    "  -- roaddamages            (Arbitrario)\n",
    "    -- data_test_coco       (Arbitrario)\n",
    "      -- train              (Obligatorio)\n",
    "        -- images           (Obligatorio)\n",
    "            -- image1.jpg\n",
    "            -- image2.jpg\n",
    "            ...\n",
    "        -- labels           (Obligatorio)\n",
    "            -- image1.txt\n",
    "            -- image2.txt\n",
    "            ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Creación de archivos .txt\n",
    "\n",
    "Teniendo la estructura de carpetas creada, se procede a llenar la carpeta `labels` con `.txt` que tienen como nombre la imagen correspondiente y que cuenta con la información de `label_codificado x y w h` de cada detección de objeto separado por enter.\n",
    "\n",
    "Para ello, se creó la siguiente implementación que toma los registros del dataframe creado en **1.**, y con dicha información, escribe el contenido correspondiente.\n",
    "\n",
    "```python\n",
    "# Por cada fila del dataframe\n",
    "\n",
    "for index_row in range(0,len(df_acum_ALL_notEmpty)):\n",
    "    # Obtener la fila\n",
    "    row=df_acum_ALL_notEmpty.iloc[index_row]\n",
    "\n",
    "    # Nombre txt\n",
    "    name_txt=get_name_file(row[\"Path\"])+\".txt\"\n",
    "\n",
    "    # Por cada label, hay un bbox\n",
    "    labels=eval(row[\"L_Label\"])\n",
    "    bboxes=eval(row[\"L_Bbox\"])\n",
    "\n",
    "    # cadena de texto auxiliar\n",
    "    str_aux=\"\"\n",
    "    for i in range(0,len(labels)):\n",
    "        bbox=bboxes[i]\n",
    "        x=bbox[0]\n",
    "        y=bbox[1]\n",
    "        w=bbox[2]\n",
    "        h=bbox[3]\n",
    "\n",
    "        lab_enc=value2Label[labels[i]]\n",
    "        if(i==len(labels)-1):\n",
    "                str_aux=str_aux+f\"{lab_enc} {x} {y} {w} {h}\"\n",
    "        else:\n",
    "            str_aux=str_aux+f\"{lab_enc} {x} {y} {w} {h}\\n\"\n",
    "\n",
    "    # Crear y escribir en el archivo\n",
    "    with open(\"./roaddamages/data_test_coco/train/labels/\"+name_txt, 'w') as archivo:\n",
    "        archivo.write(f'{str_aux}')  # Añadir cada valor en una nueva línea\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Pasar imagenes a carpeta `images`.\n",
    "\n",
    "Teniendo presente que al descomprimir las imágenes estas son guardadas por carpetas por país, se realizará una implementación para tomar cada ruta de la imagen, usando el dataframe creado en **1.** y moverla a la carpeta de `train/images`.\n",
    "\n",
    "```python\n",
    "# Pasar las imagenes con labels asociados\n",
    "for index_row in range(0,len(df_acum_ALL_notEmpty)):\n",
    "\n",
    "    # Con una imagen\n",
    "    row=df_acum_ALL_notEmpty.iloc[index_row]\n",
    "\n",
    "    # Path de la imagen origen: Son de la forma \"Img/United_States/train/images/United_States_004801.jpg\"\n",
    "    path_origen=\"./RDD2022/\"+row[\"Path\"]\n",
    "\n",
    "    # Nombre de la imagen\n",
    "    name_jpg=get_name_file(row[\"Path\"])+\".jpg\"\n",
    "\n",
    "    # Path destino\n",
    "    path_destino=\"./roaddamages/data_test_coco/train/images/\"+name_jpg\n",
    "\n",
    "    # Copiar la imagen de la carpeta original a la carpeta destino\n",
    "    shutil.copy(path_origen, path_destino)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento.\n",
    "\n",
    "Basado en el ejemplo de detección de basura en el mar [https://developer.ibm.com/tutorials/awb-train-yolo-object-detection-model-in-python/], se muestra que para realizar el entrenamiento se utiliza el siguiente comando.\n",
    "\n",
    "```cmd\n",
    "!yolo task=detect mode=train data={work_dir}/roaddamages/data_test_coco/config.yaml model=yolov8s.pt epochs=100 batch=32 lr0=.4 plots=True device=0\n",
    "```\n",
    "\n",
    "Al acabar la ejecución, en la carpeta `runs/detect` se tendrá el comportamiento del modelo final con diferentes métricas: Curva ROC, IoU, Precision, Recall, entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train data={work_dir}/roaddamages/data_test_coco/config.yaml model=yolov8s.pt epochs=100 batch=32 lr0=.4 plots=True device=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Resultados del entrenamiento\n",
    "\n",
    "Al acabar la ejecución del entrenamiento, el último modelo guardado quedó en la carpeta `runs/detect/train25/weights/best.pt`. Además, se generaron un conjunto de resultados asociados a la métricas del entrenamiento realizado. \n",
    "\n",
    "Cabe resaltar que las clases están desbalanceadas, por lo que algunas métricas tenderán a inflarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Resumen de métricas\n",
    "\n",
    "<img src=\"./train25/metricasV8pt100epocs.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusiones.\n",
    "\n",
    "* En un 75.7% aproximadamente las bounding box son verdaderas en comparación a las coordenadas reales. Aunque, para la clase D11, el valor está cerca de ser del 50% [IEEE Standards Association. (2021). IEEE Standard for Model Process for Addressing Ethical Concerns During System Design (IEEE Std 7000-2021). doi: 10.1109/IEEESTD.2021.9442728.].\n",
    "\n",
    "* En un 63.4% el modelo detecta los objetos de interés del total de todas las instancias. Además, al tener mayor precisión que recall, muestra que, a la hora de predecir, el modelo prefiere ser preciso a obtener un falso positivo, lo que puede provocar que se están perdiendo de vista, algunos objetos de interés [IEEE Standards Association. (2021). IEEE Standard for Model Process for Addressing Ethical Concerns During System Design (IEEE Std 7000-2021). doi: 10.1109/IEEESTD.2021.9442728.].\n",
    "\n",
    "* Considerando el IoU por debajo del 50% (Que para considerar un bounding box predicho como válido, debe intersectar como mínimo el 50% con el bounding box real) se obtiene un 73.1% en el balance entre Precision-Recall con este umbral, lo que sugiere que el modelo es capaz de localizar e identificar la mayoría de objetos de una imagen. Además, la mayoría de clases está por encima del valor promedio, a pesar del desbalance de instancias [Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2015). The Pascal Visual Object Classes Challenge: A Retrospective. International Journal of Computer Vision, 111(1), 98-136. doi: 10.1007/s11263-014-0733-5.].\n",
    "\n",
    "* Considerando el IoU con umbrales desde 0.5 hasta 0.95 en incrementos de 0.05, se obtiene un valor promedio de 0.49. Al ser una medida más estricta indica que el 49% de las detecciones realizadas son correctas considerando distintos niveles de umbral IoU siento la clase `Repair` la que más sobresale seguido de D43. Esto muestra, que si bien el modelo es sólido para la detección, a niveles altos de precisión, requiere de mayor ajuste o el margen de error es de casi 50% [Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., & Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. In European Conference on Computer Vision (ECCV), 2014, 740-755. doi: 10.1007/978-3-319-10602-1_48.].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2. Matriz de confusión normalizada.\n",
    "\n",
    "<img src=\"./train25/confusion_matrix_normalized.png\">\n",
    "\n",
    "##### Conclusiones.\n",
    "\n",
    "* Existe una clase más, llamada `background` que hace referencia a una clasificación que no correspone a ninguno de los tipos de daños en carretera. Con esto, notamos que la clase D10 y D00 (Las clases que más concurrencia tienen), tienen problemas para clasificarse con cierto tipo de `background`.\n",
    "\n",
    "* La clase no procesada `D0w0` se clasificó como D20, cuando debió ser clasificada como D00.\n",
    "\n",
    "* Cerca del 60% de los datos de `Block crack` y `Repair` no fueron bien clasificados.\n",
    "\n",
    "* Los daños D10, D00, D20, D40, Block crack, D01 y D11, muestran que entre un 20 a 40% de sus clasificaciones son confundidas con el fondo de la imagen, lo que indica que algunas de estas imágenes presentan mucho ruído que afecta el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo hustvl/yolos-small.\n",
    "\n",
    "Para realizar la implementación de Fine-Tunning de este modelo, se siguió el ejemplo de [https://github.com/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/Fine_tuning_YOLOS_for_object_detection_on_custom_dataset_(balloon).ipynb], donde desean clasificar si en la imagen hay balones o no.\n",
    "\n",
    "Para dejar los datos a punto se deben crear jsons tanto de entrenamiento como de validación siguiendo el formato COCO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creación de dataframe y disposición de datos.\n",
    "\n",
    "Para la creación del dataframe se utilizaron las mismas secuencia de implementaciones que en ítem **1. del modelo v8**. Sin embargo, los bounding box fueron dejados con los valores iniciales del .xml y se dejó un valor dummie para el Width y Height de la imagen, ya que, en la clase que se usará para cargar los datos, toman los tamaños originales de la imagen.\n",
    "\n",
    "El dataframe resultante posee las columnas `Path, Labels, Bboxes y TiposDanios`. Donde por cada clase hay un bounding box y un tipo de daño. Los tipos de daños son clasificaciones más generales de las clases. La anterior información es la que se desea mostrar de cara al cliente, ya que el interés principal de la aplicación es mostrar el tipo de daño general encontrado en la carretera (Esto aplica para el anterior modelo). \n",
    "\n",
    "Si bien tanto este modelo como el anterior fue entrado por la clasificación técnica del daño, a la hora de mostrar los resultados, se realizará por el tipo general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando las imágenes que poseen información de daños, se obtuvieron 25 mil registros aproximadamente, y las clases se redujeron, ya que se utilizarán las de mayor concurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Bboxes</th>\n",
       "      <th>TiposDanios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25722</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D10']</td>\n",
       "      <td>[[204.0, 494.0, 55.0, 12.0]]</td>\n",
       "      <td>['lateral linear crack']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25723</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D00', 'D00']</td>\n",
       "      <td>[[406.0, 464.0, 192.0, 172.0], [410.0, 401.0, ...</td>\n",
       "      <td>['logitudinal linear crack', 'logitudinal line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25724</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D00', 'D00']</td>\n",
       "      <td>[[300.0, 545.0, 64.0, 94.0], [356.0, 516.0, 12...</td>\n",
       "      <td>['logitudinal linear crack', 'logitudinal line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25725</th>\n",
       "      <td>Img/United_States/train/images/United_States_0...</td>\n",
       "      <td>['D00']</td>\n",
       "      <td>[[0.0, 393.0, 168.0, 102.0]]</td>\n",
       "      <td>['logitudinal linear crack']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Path          Labels  \\\n",
       "25722  Img/United_States/train/images/United_States_0...         ['D10']   \n",
       "25723  Img/United_States/train/images/United_States_0...  ['D00', 'D00']   \n",
       "25724  Img/United_States/train/images/United_States_0...  ['D00', 'D00']   \n",
       "25725  Img/United_States/train/images/United_States_0...         ['D00']   \n",
       "\n",
       "                                                  Bboxes  \\\n",
       "25722                       [[204.0, 494.0, 55.0, 12.0]]   \n",
       "25723  [[406.0, 464.0, 192.0, 172.0], [410.0, 401.0, ...   \n",
       "25724  [[300.0, 545.0, 64.0, 94.0], [356.0, 516.0, 12...   \n",
       "25725                       [[0.0, 393.0, 168.0, 102.0]]   \n",
       "\n",
       "                                             TiposDanios  \n",
       "25722                           ['lateral linear crack']  \n",
       "25723  ['logitudinal linear crack', 'logitudinal line...  \n",
       "25724  ['logitudinal linear crack', 'logitudinal line...  \n",
       "25725                       ['logitudinal linear crack']  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path en caso de estar en google drive\n",
    "google_drive=False\n",
    "p_metadata_3=\"df_metada_acum.xlsx\"\n",
    "df_metadata_acum=pd.read_excel(p_metadata_3)\n",
    "df_metadata_acum.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "df_metadata_acum.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordemos que en este caso particular, la carpeta segmentada por paises está ubicada en RDD2022/Img\n",
    "\n",
    "df_metadata_acum[\"Path\"]=list(map(lambda x: \"RDD2022/\"+x,list(df_metadata_acum[\"Path\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Bboxes</th>\n",
       "      <th>TiposDanios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25722</th>\n",
       "      <td>RDD2022/Img/United_States/train/images/United_...</td>\n",
       "      <td>['D10']</td>\n",
       "      <td>[[204.0, 494.0, 55.0, 12.0]]</td>\n",
       "      <td>['lateral linear crack']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25723</th>\n",
       "      <td>RDD2022/Img/United_States/train/images/United_...</td>\n",
       "      <td>['D00', 'D00']</td>\n",
       "      <td>[[406.0, 464.0, 192.0, 172.0], [410.0, 401.0, ...</td>\n",
       "      <td>['logitudinal linear crack', 'logitudinal line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25724</th>\n",
       "      <td>RDD2022/Img/United_States/train/images/United_...</td>\n",
       "      <td>['D00', 'D00']</td>\n",
       "      <td>[[300.0, 545.0, 64.0, 94.0], [356.0, 516.0, 12...</td>\n",
       "      <td>['logitudinal linear crack', 'logitudinal line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25725</th>\n",
       "      <td>RDD2022/Img/United_States/train/images/United_...</td>\n",
       "      <td>['D00']</td>\n",
       "      <td>[[0.0, 393.0, 168.0, 102.0]]</td>\n",
       "      <td>['logitudinal linear crack']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Path          Labels  \\\n",
       "25722  RDD2022/Img/United_States/train/images/United_...         ['D10']   \n",
       "25723  RDD2022/Img/United_States/train/images/United_...  ['D00', 'D00']   \n",
       "25724  RDD2022/Img/United_States/train/images/United_...  ['D00', 'D00']   \n",
       "25725  RDD2022/Img/United_States/train/images/United_...         ['D00']   \n",
       "\n",
       "                                                  Bboxes  \\\n",
       "25722                       [[204.0, 494.0, 55.0, 12.0]]   \n",
       "25723  [[406.0, 464.0, 192.0, 172.0], [410.0, 401.0, ...   \n",
       "25724  [[300.0, 545.0, 64.0, 94.0], [356.0, 516.0, 12...   \n",
       "25725                       [[0.0, 393.0, 168.0, 102.0]]   \n",
       "\n",
       "                                             TiposDanios  \n",
       "25722                           ['lateral linear crack']  \n",
       "25723  ['logitudinal linear crack', 'logitudinal line...  \n",
       "25724  ['logitudinal linear crack', 'logitudinal line...  \n",
       "25725                       ['logitudinal linear crack']  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata_acum.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Separación de datos en Train y Val.\n",
    "\n",
    "En este caso, es requerido para este modelo tener un conjunto de datos de entrenamiento y un junto de datos de prueba. Para la construcción de este, se implementó la siguiente función que parte el conjunto de datos en un porcentaje arbitrario. La proporción de datos de entrenamiento y prueba, al ser desbalanceado, se optó por 90% de entrenamiento y 10% de pruebas, esto es, buscando minimizar el desbalanceo. Esto se podría mejorar, realizando una separación estratificado, pero debido a que la variable objetivo es una lista, se optó no usarlo por practicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para filtrar el dataframe por algun país en específico.\n",
    "\n",
    "def get_df_bypath(df,constraint=\"\"):\n",
    "    df_filtered=df.copy()[df.copy()[\"Path\"].str.contains(constraint)]\n",
    "    return df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para separar el conjunto de datos de entrenamiento y prueba dado una proporción específica\n",
    "def train_test_split_RDD(df, prop=0.2):\n",
    "    X = df[[\"Path\",\"Labels\",\"Bboxes\",\"TiposDanios\"]]\n",
    "    X_train, X_test = train_test_split(X, test_size=prop, random_state=18022022)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, y luego de una exploración de las imágenes, se observó que las imágenes de mejor calidad (donde hay menos ruido por paisajes) son las de Japón, además que es el país que más imágenes tiene con 9 mil aproximadamente. Por otra parte, las imágenes de India y Noruega, son las que mayor presentan ruído en sus imágenes y puede afectar el entrenamiento de manera negativa en caso de no incluirlas. Por esta razón, se decidió unir los dataframes de los países de Japón, India y China, y se realizó una separación de 10% de datos en prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Bboxes</th>\n",
       "      <th>TiposDanios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9309</th>\n",
       "      <td>RDD2022/Img/Japan/train/images/Japan_013131.jpg</td>\n",
       "      <td>['D20']</td>\n",
       "      <td>[[475.0, 505.0, 90.0, 38.0]]</td>\n",
       "      <td>['alligator crack']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9310</th>\n",
       "      <td>RDD2022/Img/Japan/train/images/Japan_013132.jpg</td>\n",
       "      <td>['D00', 'D43', 'D20']</td>\n",
       "      <td>[[542.0, 275.0, 58.0, 61.0], [189.0, 82.0, 221...</td>\n",
       "      <td>['logitudinal linear crack', 'other corruption...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Path                 Labels  \\\n",
       "9309  RDD2022/Img/Japan/train/images/Japan_013131.jpg                ['D20']   \n",
       "9310  RDD2022/Img/Japan/train/images/Japan_013132.jpg  ['D00', 'D43', 'D20']   \n",
       "\n",
       "                                                 Bboxes  \\\n",
       "9309                       [[475.0, 505.0, 90.0, 38.0]]   \n",
       "9310  [[542.0, 275.0, 58.0, 61.0], [189.0, 82.0, 221...   \n",
       "\n",
       "                                            TiposDanios  \n",
       "9309                                ['alligator crack']  \n",
       "9310  ['logitudinal linear crack', 'other corruption...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_country=\"Japan\"\n",
    "df_j=get_df_bypath(df_metadata_acum,constraint=path_country)\n",
    "df_j.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Bboxes</th>\n",
       "      <th>TiposDanios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>RDD2022/Img/Norway/train/images/Norway_008156.jpg</td>\n",
       "      <td>['D00', 'D00', 'D00', 'D00']</td>\n",
       "      <td>[[345.57, 1374.61, 720.8100000000002, 604.1000...</td>\n",
       "      <td>['logitudinal linear crack', 'logitudinal line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>RDD2022/Img/Norway/train/images/Norway_008159.jpg</td>\n",
       "      <td>['D10', 'D00', 'D00']</td>\n",
       "      <td>[[1299.8, 1211.93, 229.04999999999995, 17.9299...</td>\n",
       "      <td>['lateral linear crack', 'logitudinal linear c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Path  \\\n",
       "2912  RDD2022/Img/Norway/train/images/Norway_008156.jpg   \n",
       "2913  RDD2022/Img/Norway/train/images/Norway_008159.jpg   \n",
       "\n",
       "                            Labels  \\\n",
       "2912  ['D00', 'D00', 'D00', 'D00']   \n",
       "2913         ['D10', 'D00', 'D00']   \n",
       "\n",
       "                                                 Bboxes  \\\n",
       "2912  [[345.57, 1374.61, 720.8100000000002, 604.1000...   \n",
       "2913  [[1299.8, 1211.93, 229.04999999999995, 17.9299...   \n",
       "\n",
       "                                            TiposDanios  \n",
       "2912  ['logitudinal linear crack', 'logitudinal line...  \n",
       "2913  ['lateral linear crack', 'logitudinal linear c...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_country=\"Norway\"\n",
    "df_norway=get_df_bypath(df_metadata_acum,constraint=path_country)\n",
    "df_norway.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Bboxes</th>\n",
       "      <th>TiposDanios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>RDD2022/Img/India/train/images/India_009889.jpg</td>\n",
       "      <td>['D43']</td>\n",
       "      <td>[[102.0, 488.0, 421.0, 68.0]]</td>\n",
       "      <td>['other corruption']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>RDD2022/Img/India/train/images/India_009890.jpg</td>\n",
       "      <td>['D40', 'D40']</td>\n",
       "      <td>[[238.0, 500.0, 56.0, 41.0], [301.0, 455.0, 56...</td>\n",
       "      <td>['other corruption', 'other corruption']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Path          Labels  \\\n",
       "3769  RDD2022/Img/India/train/images/India_009889.jpg         ['D43']   \n",
       "3770  RDD2022/Img/India/train/images/India_009890.jpg  ['D40', 'D40']   \n",
       "\n",
       "                                                 Bboxes  \\\n",
       "3769                      [[102.0, 488.0, 421.0, 68.0]]   \n",
       "3770  [[238.0, 500.0, 56.0, 41.0], [301.0, 455.0, 56...   \n",
       "\n",
       "                                   TiposDanios  \n",
       "3769                      ['other corruption']  \n",
       "3770  ['other corruption', 'other corruption']  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_country=\"India\"\n",
    "df_india=get_df_bypath(df_metadata_acum,constraint=path_country)\n",
    "df_india.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Bboxes</th>\n",
       "      <th>TiposDanios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>RDD2022/Img/Norway/train/images/Norway_008156.jpg</td>\n",
       "      <td>['D00', 'D00', 'D00', 'D00']</td>\n",
       "      <td>[[345.57, 1374.61, 720.8100000000002, 604.1000...</td>\n",
       "      <td>['logitudinal linear crack', 'logitudinal line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>RDD2022/Img/Norway/train/images/Norway_008159.jpg</td>\n",
       "      <td>['D10', 'D00', 'D00']</td>\n",
       "      <td>[[1299.8, 1211.93, 229.04999999999995, 17.9299...</td>\n",
       "      <td>['lateral linear crack', 'logitudinal linear c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Path  \\\n",
       "15994  RDD2022/Img/Norway/train/images/Norway_008156.jpg   \n",
       "15995  RDD2022/Img/Norway/train/images/Norway_008159.jpg   \n",
       "\n",
       "                             Labels  \\\n",
       "15994  ['D00', 'D00', 'D00', 'D00']   \n",
       "15995         ['D10', 'D00', 'D00']   \n",
       "\n",
       "                                                  Bboxes  \\\n",
       "15994  [[345.57, 1374.61, 720.8100000000002, 604.1000...   \n",
       "15995  [[1299.8, 1211.93, 229.04999999999995, 17.9299...   \n",
       "\n",
       "                                             TiposDanios  \n",
       "15994  ['logitudinal linear crack', 'logitudinal line...  \n",
       "15995  ['lateral linear crack', 'logitudinal linear c...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union de los dataframes anteriores\n",
    "\n",
    "# Concatenando por filas, ya que poseen las mismas columnas\n",
    "df_tv = pd.concat([df_j,df_india,df_norway], axis=0).reset_index(drop=True)\n",
    "df_tv.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15996 14396 1600\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test= train_test_split_RDD(df_tv,0.1)\n",
    "print(len(df_tv), len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Creación de jsons.\n",
    "\n",
    "Usando el ejemplo del Fine-Tunning para clasificar balones, notamos que se crean jsons con una estructura específica. Es por esto, que se decidió crear los jsons a través de una implementación tendiendo presente la siguiente estructura, donde lso datos que están fijos, es porque así fueron tomados del ejemplo guía.\n",
    "\n",
    "```python\n",
    "# Obtener la fecha y hora actuales\n",
    "fecha_actual = datetime.now()\n",
    "\n",
    "# Formatear la fecha y hora al formato yyyy-mm-dd hh:mm:ss\n",
    "fecha_formateada = fecha_actual.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "```\n",
    "\n",
    "```json\n",
    "custom_json={\n",
    "    \"info\": {\n",
    "            \"description\": \"Example Dataset\", \n",
    "            \"url\": \"https://github.com/waspinator/pycococreator\", \n",
    "            \"version\": \"0.1.0\", \n",
    "            \"year\": 2018, \n",
    "            \"contributor\": \"waspinator\", \n",
    "            \"date_created\": f\"{fecha_formateada}\"\n",
    "    },\n",
    "    \"licenses\": [\n",
    "            {\n",
    "                \"id\": 1, \n",
    "                \"name\": \"Attribution-NonCommercial-ShareAlike License\", \n",
    "                \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n",
    "            }\n",
    "    ],\n",
    "    \"categories\": [\n",
    "            {\"id\": 0, \"name\": \"D10\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 1, \"name\": \"D00\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 2, \"name\": \"D20\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 3, \"name\": \"D40\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 4, \"name\": \"D44\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 5, \"name\": \"D01\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 6, \"name\": \"D11\", \"supercategory\": \"N/A\"}, \n",
    "            {\"id\": 7, \"name\": \"D43\", \"supercategory\": \"N/A\"}\n",
    "        ],\n",
    "    \"images\": [\n",
    "            {\"id\": 0, \"file_name\": \"India/train/images/India_005399.jpg\", \"width\": 600, \"height\": 600, \"date_captured\": \"2024-09-05 16:16:08\", \"license\": 1, \"coco_url\": \"\", \"flickr_url\": \"\"}, \n",
    "            {\"id\": 1, \"file_name\": \"Japan/train/images/Japan_005085.jpg\", \"width\": 600, \"height\": 600, \"date_captured\": \"2024-09-05 16:16:08\", \"license\": 1, \"coco_url\": \"\", \"flickr_url\": \"\"},\n",
    "            ... \n",
    "        ],\n",
    "    \"annotations\":[\n",
    "        {\"id\": 0, \"image_id\": 0, \"category_id\": 2, \"iscrowd\": 0, \"area\": 0, \"bbox\": [173, 518, 547, 197], \"segmentation\":[[0]]}, \n",
    "        {\"id\": 1, \"image_id\": 0, \"category_id\": 3, \"iscrowd\": 0, \"area\": 0, \"bbox\": [541, 610, 82, 50], \"segmentation\": [[0]]}, {\"id\": 2, \"image_id\": 1, \"category_id\": 1, \"iscrowd\": 0, \"area\": 0, \"bbox\": [130, 356, 118, 103], \"segmentation\": [[0]]}, \n",
    "        {\"id\": 3, \"image_id\": 1, \"category_id\": 1, \"iscrowd\": 0, \"area\": 0, \"bbox\": [319, 336, 87, 95], \"segmentation\": [[0]]}, \n",
    "        ... \n",
    "        ]\n",
    "}\n",
    "```\n",
    "Notemos que los registros se separan por id, tanto las imagenes como los bounding boxes. Para lograr dicho formato, se realiza la siguiente implementación, donde, al guardarse ambos archivos, se debe **tener presente la ruta raíz de este:** `RDD2022/Img/`. Esto es, porque a partir esa ruta raíz, se buscará la imagen para el entrenamiento, por lo que sino está en dicha ruta, saldrá un error en las rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener nombre del archivo dado un path\n",
    "def get_name_file(path):\n",
    "    l=path.split(\"/\")\n",
    "    return l[len(l)-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columna para asignar los valores de cada categoria\n",
    "LABEL_MAPEADO={'D10':0, 'D00':1, 'D20':2,'D40':3, 'D44':4, 'D01':5,'D11':6, 'D43': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D10', 'D00', 'D20', 'D40', 'D44', 'D01', 'D11', 'D43']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS=[k for k, v in LABEL_MAPEADO.items()]\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:16:08\n",
      "Cargar Info\n",
      "Cargar Licenses\n",
      "Cargar Categories\n",
      "Cargar images\n",
      "Cargar Annotations\n",
      "Guardar Json: RDD2022/Img/custom_train.json en RDD2022/Img/, esta ruta la debe ingresar en el parametro img_folder\n",
      "\n",
      "2024-09-05 16:16:15\n",
      "Cargar Info\n",
      "Cargar Licenses\n",
      "Cargar Categories\n",
      "Cargar images\n",
      "Cargar Annotations\n",
      "Guardar Json: RDD2022/Img/custom_val.json en RDD2022/Img/, esta ruta la debe ingresar en el parametro img_folder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_json(df, save_json=\"\", train=True):\n",
    "    # Obtener la fecha y hora actuales\n",
    "    fecha_actual = datetime.now()\n",
    "\n",
    "    # Formatear la fecha y hora al formato yyyy-mm-dd hh:mm:ss\n",
    "    fecha_formateada = fecha_actual.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(fecha_formateada)\n",
    "\n",
    "    print(\"Cargar Info\")\n",
    "    info={\"description\": \"Example Dataset\", \n",
    "      \"url\": \"https://github.com/waspinator/pycococreator\", \n",
    "      \"version\": \"0.1.0\", \n",
    "      \"year\": 2018, \n",
    "      \"contributor\": \"waspinator\", \n",
    "      \"date_created\": f\"{fecha_formateada}\"}\n",
    "    \n",
    "    print(\"Cargar Licenses\")\n",
    "    licenses=[{\"id\": 1, \"name\": \"Attribution-NonCommercial-ShareAlike License\", \n",
    "            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"}]\n",
    "\n",
    "    print(\"Cargar Categories\")\n",
    "    categories=[]\n",
    "    l_categories=LABELS\n",
    "    for i in range(len(l_categories)):\n",
    "        categories.append({\"id\":i,\n",
    "                        \"name\":l_categories[i],\n",
    "                        \"supercategory\":\"N/A\"})\n",
    "        \n",
    "    print(\"Cargar images\")\n",
    "    images=[]\n",
    "    l_paths=list(map(lambda x: get_name_file(x)+\".jpg\",df[\"Path\"]))\n",
    "    paths_f=[]\n",
    "    for p in l_paths:\n",
    "        \n",
    "        if(\"China_Drone\" in p):\n",
    "            paths_f.append(\"China_Drone/train/images/\"+p)\n",
    "        elif(\"China_MotorBike\" in p):\n",
    "            paths_f.append(\"China_MotorBike/train/images/\"+p)\n",
    "        elif(\"Czech\" in p):\n",
    "            paths_f.append(\"Czech/train/images/\"+p)\n",
    "        elif(\"India\" in p):\n",
    "            paths_f.append(\"India/train/images/\"+p)\n",
    "        elif(\"Japan\" in p):\n",
    "            paths_f.append(\"Japan/train/images/\"+p)\n",
    "        elif(\"Norway\" in p):\n",
    "            paths_f.append(\"Norway/train/images/\"+p)\n",
    "        elif(\"United_States\" in p):\n",
    "            paths_f.append(\"United_States/train/images/\"+p)\n",
    "    for i in range(len(l_paths)):\n",
    "        images.append({\"id\": i,\n",
    "                    \"file_name\": paths_f[i],\n",
    "                    \"width\": 600,\n",
    "                    \"height\": 600,\n",
    "                    \"date_captured\": \"\"+str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
    "                    \"license\": 1,\n",
    "                    \"coco_url\": \"\", \n",
    "                    \"flickr_url\": \"\"\n",
    "                    })\n",
    "    \n",
    "    print(\"Cargar Annotations\")\n",
    "    # Annotations\n",
    "    ides=[i for i in range(len(df))]\n",
    "    df[\"Id\"]=ides\n",
    "\n",
    "    annotations=[]\n",
    "\n",
    "    # Por cada fila\n",
    "\n",
    "    id_ann=0\n",
    "    for index_fila in range(len(df)):\n",
    "        fila = df.iloc[index_fila]\n",
    "\n",
    "        # Obtengo los valores unitarios: index\n",
    "        id_fila=fila[\"Id\"]\n",
    "\n",
    "        # Obtener lista de bbox, labels, TiposDanios\n",
    "        fila_list_bbox=eval(fila[\"Bboxes\"])\n",
    "        fila_list_labels=eval(fila[\"Labels\"])\n",
    "        fila_list_danios=eval(fila[\"TiposDanios\"])\n",
    "\n",
    "        for id_danio in range(len(fila_list_danios)):\n",
    "            annotations.append({\n",
    "                \"id\": int(id_ann),\n",
    "                \"image_id\": int(id_fila),\n",
    "                \"category_id\": int(LABEL_MAPEADO[fila_list_labels[id_danio]]),\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": 0,\n",
    "                \"bbox\": [int(num) for num in fila_list_bbox[id_danio]],\n",
    "                \"segmentation\": [[0]]\n",
    "            })\n",
    "            id_ann+=1\n",
    "\n",
    "    custom_train={\"info\":info,\n",
    "              \"licenses\":licenses,\n",
    "              \"categories\":categories,\n",
    "              \"images\":images,\n",
    "              \"annotations\": annotations}\n",
    "    \n",
    "    # Nombre del archivo JSON que se va a crear\n",
    "    if(train):\n",
    "        filename = save_json+\"custom_train.json\"\n",
    "    else:\n",
    "        filename = save_json+\"custom_val.json\"\n",
    "\n",
    "    print(f\"Guardar Json: {filename} en {save_json}, esta ruta la debe ingresar en el parametro img_folder\")\n",
    "    # Guardar el diccionario como un archivo .json\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(custom_train, file)  # El parámetro indent=4 es opcional, solo para que el JSON se vea más legible\n",
    "    print()\n",
    "    return custom_train, save_json\n",
    "\n",
    "r=create_json(X_train,save_json=\"RDD2022/Img/\",train=True)\n",
    "r=create_json(X_test,save_json=\"RDD2022/Img/\",train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Cargado de datos.\n",
    "\n",
    "Para cargar los datos, se utiliza la clase `AutoFeatureExtractor` de la librería `transformers`, donde a partir de la arquitectura del modelo (en este caso \"hustvl/yolos-small\"), dispone un generador de formatos que facilita la obtención de los datos a partir de archivos .json siguiendo la estructura mostrada en el **ítem 1.2. de este modelo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\yolos\\feature_extraction_yolos.py:38: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"hustvl/yolos-small\", size=512, max_size=864)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se debe asignar en el parámetro `img_folder` la ruta donde se cargarán los .json que contiene la metadata dispuesta. Para eso, se utiliza la clase CocoDetection(), que tiene preparado el formato establecido anteriormente, y facilita la entrada a los datos de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"custom_train.json\" if train else \"custom_val.json\")\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoDetection(img_folder=f'RDD2022/Img', feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder=f'RDD2022/Img', feature_extractor=feature_extractor, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 14396\n",
      "Number of val examples: 1600\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de datos instanciados con la clase CocoDetection\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of val examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, se prepara un dataloader que contendrá los datos dispuestos para los procesos de entrenamiento y pruebas. Utilizando un batch de 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = feature_extractor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=3)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entrenamiento.\n",
    "\n",
    "La configuración del modelo para realizar el entrenamiento, es dado a un modulo `Lightning` de la librería `pytorch_lightning`, donde se buscará usar CUDA para realizar un entrenamiento menos demorado. Así que se configura una clase para instanciar la estructura del modelo pre-entrenado y se utilizará el optimizador Adam ponderado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         self.model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-small\", \n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         self.lr = lr\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values):\n",
    "       outputs = self.model(pixel_values=pixel_values)\n",
    "\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-small and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 384]) in the checkpoint and torch.Size([9, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Detr(lr=2.5e-1, weight_decay=1e-4)\n",
    "\n",
    "outputs = model(pixel_values=batch['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gradient_clip_val=0.1, \n",
    "                  accumulate_grad_batches=4,\n",
    "                  max_epochs=20)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluación del modelo.\n",
    "\n",
    "Siguiendo las implementaciones anteriores, se podría usar cualquier conjunto de datos para evaluarlo. Usando el conjunto de datos en prueba (val), se obtienen los siguientes resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe realizar un acceso a un repositorio en GitHub para poder realizar la verificación.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/facebookresearch/detr.git\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr\n",
    "\n",
    "from detr.datasets import get_coco_api_from_dataset\n",
    "\n",
    "base_ds = get_coco_api_from_dataset(val_dataset) # this is actually just calling the coco attribute\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr\n",
    "\n",
    "from datasets.coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, se obtiene los siguientes resultados.\n",
    "\n",
    "<img src=\"metricasIoUdataJin.png\">\n",
    "\n",
    "##### Conclusiones.\n",
    "\n",
    "* Notamos que las metricas obtenidas, no alcanzan a satisfacer las necesidades, en comparación con la del primer modelo.\n",
    "\n",
    "* En temas computacionales, la duración de entrenamiento de este modelo fue mayor duración por epoc, durando 12 min por epoch, en comparación del modelo v8 con una duración de 8 min.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
